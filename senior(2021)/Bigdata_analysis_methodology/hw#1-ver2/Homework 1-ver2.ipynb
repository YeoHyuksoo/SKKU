{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Homework 1 (Due: Mar. 17, 2021 (11:59 PM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Yeo hyuksoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student ID: 2016312761\n",
    "\n",
    "Late submission Days used here: 0 (if there is, please modify here)\n",
    "\n",
    "[Please SUBMIT (1) YOUR IPYNB AND (2) PDF (please use FILE/DOWNLOAD AS/PDF or PRINT PREVIEW/PRINT AS PDF with \"printed output\") TO iCampus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "For this homework you cannot use the python library scikit-learn (sklearn). \n",
    "You can use the python package BeautifulSoup to parse web pages.\n",
    "\n",
    "In this assignment you will retrieve and parse webpages. The text file \"urls.txt\" contains a list of urls for the webpages to be parsed. Each line in the text file corresponds to a url. Use BeautifulSoup to fetch each webpage.\n",
    "\n",
    "Note: For all questions, the words should be converted to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q1: Part 1 (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Parse the first webpage document to retrieve the text enclosed in paragraph tags, find the words that end in \"ing\", and count how many times each word appears. \n",
    "\n",
    "Sort these words in decreasing order of frequency, write the words (along with their corresponding frequencies) in (1) this Notebook and (2) an output file named \"Q1_Part1.txt\". The most frequent word should appear at the top and the least frequent word at the end, and the format of the output file should be:\n",
    "word TAB frequency\n",
    "\n",
    "If two words have the same frequency, use the alphabetical order of words for outputs.\n",
    "\n",
    "Example:\n",
    "\n",
    "sorting\t10\n",
    "training\t8\n",
    "broadening\t6\n",
    "extracting\t3\n",
    "evergrowing\t2\n",
    "coming\t1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning\t5\n",
      "mining\t3\n",
      "beijing\t2\n",
      "computing\t2\n",
      "accounting\t1\n",
      "analyzing\t1\n",
      "applying\t1\n",
      "becoming\t1\n",
      "being\t1\n",
      "breaking\t1\n",
      "changing\t1\n",
      "combining\t1\n",
      "creating\t1\n",
      "describing\t1\n",
      "developing\t1\n",
      "drawing\t1\n",
      "during\t1\n",
      "emerging\t1\n",
      "enabling\t1\n",
      "everything\t1\n",
      "extracting\t1\n",
      "finding\t1\n",
      "formulating\t1\n",
      "growing\t1\n",
      "including\t1\n",
      "managing\t1\n",
      "preparing\t1\n",
      "presenting\t1\n",
      "reflecting\t1\n",
      "training\t1\n",
      "turing\t1\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "\n",
    "r = urllib.request.urlopen('https://en.wikipedia.org/wiki/Data_science').read()\n",
    "soup = BeautifulSoup(r)\n",
    "\n",
    "word_dict={}\n",
    "words1 = []\n",
    "words2 = []\n",
    "words3 = []\n",
    "words4 = []\n",
    "words5 = []\n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "for p in paragraphs:\n",
    "    pText = p.get_text().split(' ')\n",
    "    for w in pText:\n",
    "        w = w.lower()\n",
    "        if 'ing' in w:\n",
    "            if len(w)>w.index('ing')+3 and w[w.index('ing')+3].isalpha():\n",
    "                continue\n",
    "        pattern = re.compile(r\"\\w*ing\")\n",
    "        target = re.findall(pattern, w)\n",
    "        if target:\n",
    "            target = str(target)[2:len(target)-3]\n",
    "            if target not in word_dict:\n",
    "                word_dict[target] = 1\n",
    "            else:\n",
    "                word_dict[target] += 1\n",
    "\n",
    "word_dict = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for key, value in word_dict:\n",
    "    if value == 1:\n",
    "        words1.append(key)\n",
    "    elif value == 2:\n",
    "        words2.append(key)\n",
    "    elif value == 3:\n",
    "        words3.append(key)\n",
    "    elif value == 4:\n",
    "        words4.append(key)\n",
    "    else:\n",
    "        words5.append(key)\n",
    "\n",
    "words1.sort()\n",
    "words2.sort()\n",
    "words3.sort()\n",
    "words4.sort()\n",
    "words5.sort()\n",
    "\n",
    "f = open('Q1_Part1.txt', 'w')\n",
    "\n",
    "for w in words5:\n",
    "    res = w+'\\t5\\n'\n",
    "    f.write(res)\n",
    "    print(res, end=\"\")\n",
    "\n",
    "for w in words4:\n",
    "    res = w+'\\t4\\n'\n",
    "    f.write(res)\n",
    "    print(res, end=\"\")\n",
    "    \n",
    "for w in words3:\n",
    "    res = w+'\\t3\\n'\n",
    "    f.write(res)\n",
    "    print(res, end=\"\")\n",
    "    \n",
    "for w in words2:\n",
    "    res = w+'\\t2\\n'\n",
    "    f.write(res)\n",
    "    print(res, end=\"\")\n",
    "    \n",
    "for w in words1:\n",
    "    res = w+'\\t1\\n'\n",
    "    f.write(res)\n",
    "    print(res, end=\"\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q1: Part 2 (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Stop words are natural language words which have very little meaning, such as \"and\", \"the\", \"a\", \"an\", and similar words.\n",
    "\n",
    "Repeat Part 1, but before counting, remove the stop words given in the file \"stop_words.txt\". The ouput for Part 2 should have the same format as Part 1, and should be written to (1) this Notebook and (2) an output file named \"Q1_Part2.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning\t5\n",
      "mining\t3\n",
      "beijing\t2\n",
      "computing\t2\n",
      "accounting\t1\n",
      "analyzing\t1\n",
      "applying\t1\n",
      "breaking\t1\n",
      "changing\t1\n",
      "combining\t1\n",
      "creating\t1\n",
      "describing\t1\n",
      "developing\t1\n",
      "drawing\t1\n",
      "emerging\t1\n",
      "enabling\t1\n",
      "extracting\t1\n",
      "finding\t1\n",
      "formulating\t1\n",
      "growing\t1\n",
      "including\t1\n",
      "managing\t1\n",
      "preparing\t1\n",
      "presenting\t1\n",
      "reflecting\t1\n",
      "training\t1\n",
      "turing\t1\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "\n",
    "r = urllib.request.urlopen('https://en.wikipedia.org/wiki/Data_science').read()\n",
    "soup = BeautifulSoup(r)\n",
    "\n",
    "word_dict={}\n",
    "words1 = []\n",
    "words2 = []\n",
    "words3 = []\n",
    "words4 = []\n",
    "words5 = []\n",
    "\n",
    "f = open('stop_words.txt', 'r')\n",
    "stop_list = f.readlines()\n",
    "for i in range(0, len(stop_list), 1):\n",
    "    stop_list[i] = stop_list[i][:len(stop_list[i])-1]\n",
    "f.close()\n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "for p in paragraphs:\n",
    "    pText = p.get_text().split(' ')\n",
    "    for w in pText:\n",
    "        w = w.lower()\n",
    "        if 'ing' in w:\n",
    "            if len(w)>w.index('ing')+3 and w[w.index('ing')+3].isalpha():\n",
    "                continue\n",
    "        pattern = re.compile(r\"\\w*ing\")\n",
    "        target = re.findall(pattern, w)\n",
    "        if target:\n",
    "            target = str(target)[2:len(target)-3]\n",
    "            if target not in stop_list:\n",
    "                if target not in word_dict:\n",
    "                    word_dict[target] = 1\n",
    "                else:\n",
    "                    word_dict[target] += 1\n",
    "                \n",
    "word_dict = sorted(word_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "for key, value in word_dict:\n",
    "    if value == 1:\n",
    "        words1.append(key)\n",
    "    elif value == 2:\n",
    "        words2.append(key)\n",
    "    elif value == 3:\n",
    "        words3.append(key)\n",
    "    elif value == 4:\n",
    "        words4.append(key)\n",
    "    else:\n",
    "        words5.append(key)\n",
    "\n",
    "words1.sort()\n",
    "words2.sort()\n",
    "words3.sort()\n",
    "words4.sort()\n",
    "words5.sort()\n",
    "\n",
    "f = open('Q1_Part2.txt', 'w')\n",
    "\n",
    "for w in words5:\n",
    "    res = w+'\\t5\\n'\n",
    "    f.write(res)\n",
    "    print(res, end=\"\")\n",
    "\n",
    "for w in words4:\n",
    "    res = w+'\\t4\\n'\n",
    "    f.write(res)\n",
    "    print(res, end=\"\")\n",
    "    \n",
    "for w in words3:\n",
    "    res = w+'\\t3\\n'\n",
    "    f.write(res)\n",
    "    print(res, end=\"\")\n",
    "    \n",
    "for w in words2:\n",
    "    res = w+'\\t2\\n'\n",
    "    f.write(res)\n",
    "    print(res, end=\"\")\n",
    "    \n",
    "for w in words1:\n",
    "    res = w+'\\t1\\n'\n",
    "    f.write(res)\n",
    "    print(res, end=\"\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Again, parse the first webpage document, but this time find and count all outgoing links to other webpages, and write the output to (1) this Notebook and (2) a file named \"Q2.txt\", with each url on a new line.\n",
    "\n",
    "The format of the output file should: number of outgoing urls in the first line, followed by each url on a new line. \n",
    "For example, \n",
    "4\n",
    "https://eng.skku.edu/eng/edu/education.do\n",
    "https://eng.skku.edu/eng/Research/industry/researchStory.do\n",
    "https://eng.skku.edu/eng/Univ-Industry/Research-Business-Found/FactsandFigures.do\n",
    "https://eng.skku.edu/eng/CampusLife/support/employment.do\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "https://arxiv.org/list/cs.LG/recent\n",
      "https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&action=edit\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&action=edit\n",
      "http://cacm.acm.org/magazines/2013/12/169933-data-science-and-prediction/fulltext\n",
      "https://doi.org/10.1145%2F2500499\n",
      "https://api.semanticscholar.org/CorpusID:6107147\n",
      "https://web.archive.org/web/20141109113411/http://cacm.acm.org/magazines/2013/12/169933-data-science-and-prediction/fulltext\n",
      "http://simplystatistics.org/2013/12/12/the-key-word-in-data-science-is-not-data-it-is-science/\n",
      "https://web.archive.org/web/20140102194117/http://simplystatistics.org/2013/12/12/the-key-word-in-data-science-is-not-data-it-is-science/\n",
      "https://www.springer.com/book/9784431702085\n",
      "https://doi.org/10.1007%2F978-4-431-65950-1_3\n",
      "https://books.google.com/books?id=oGs_AQAAIAAJ\n",
      "https://web.archive.org/web/20170320193019/https://books.google.com/books?id=oGs_AQAAIAAJ\n",
      "https://doi.org/10.1126%2Fscience.1170411\n",
      "https://api.semanticscholar.org/CorpusID:9743327\n",
      "http://www.datascienceassn.org/about-data-science\n",
      "https://www.oreilly.com/library/view/doing-data-science/9781449363871/ch01.html\n",
      "https://medriscoll.com/post/4740157098/the-three-sexy-skills-of-data-geeks\n",
      "https://flowingdata.com/2009/06/04/rise-of-the-data-scientist/\n",
      "https://benfry.com/phd/dissertation/2.html\n",
      "https://magazine.amstat.org/blog/2015/10/01/asa-statement-on-the-role-of-statistics-in-data-science/\n",
      "https://web.archive.org/web/20190620184935/https://magazine.amstat.org/blog/2015/10/01/asa-statement-on-the-role-of-statistics-in-data-science/\n",
      "https://www.statisticsviews.com/article/nate-silver-what-i-need-from-statisticians/\n",
      "http://priceonomics.com/whats-the-difference-between-data-science-and/\n",
      "https://doi.org/10.1145%2F2500499\n",
      "https://api.semanticscholar.org/CorpusID:6107147\n",
      "https://statmodeling.stat.columbia.edu/2013/11/14/statistics-least-important-part-data-science/\n",
      "https://www.datasciencecentral.com/profiles/blogs/data-science-without-statistics-is-possible-even-desirable\n",
      "http://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf\n",
      "https://www2.isye.gatech.edu/~jeffwu/publications/fazhan.pdf\n",
      "https://www.mdpi.com/2504-2289/2/2/14\n",
      "https://doi.org/10.3390%2Fbdcc2020014\n",
      "https://doi.org/10.1145%2F3076253\n",
      "https://doi.org/10.1145%2F3076253\n",
      "http://www2.isye.gatech.edu/~jeffwu/presentations/datascience.pdf\n",
      "https://www.forbes.com/sites/gilpress/2013/05/28/a-very-short-history-of-data-science/\n",
      "https://www.stat.purdue.edu/~wsc/\n",
      "https://magazine.amstat.org/blog/2016/06/01/datascience-2/\n",
      "https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century\n",
      "https://www.nsf.gov/pubs/2005/nsb0540/\n",
      "https://www.forbes.com/sites/gilpress/2013/08/19/data-science-whats-the-half-life-of-a-buzzword/\n",
      "https://www.forbes.com/sites/peterpham/2015/08/28/the-impacts-of-big-data-that-you-may-not-have-heard-of/\n",
      "https://towardsdatascience.com/how-data-science-will-impact-future-of-businesses-7f11f5699c4d\n",
      "https://sites.engineering.ucsb.edu/~shell/che210d/python.pdf\n",
      "https://cran.r-project.org/doc/FAQ/R-FAQ.html#What-is-R_003f\n",
      "https://www.wired.com/2014/07/a-drag-and-drop-toolkit-that-lets-anyone-create-interactive-maps/\n",
      "https://en.wikipedia.org/w/index.php?title=Template:Data&action=edit\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&oldid=1011117061\n",
      "https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q2374463\n",
      "https://commons.wikimedia.org/wiki/Category:Data_science\n",
      "https://ar.wikipedia.org/wiki/%D8%B9%D9%84%D9%85_%D8%A7%D9%84%D8%A8%D9%8A%D8%A7%D9%86%D8%A7%D8%AA\n",
      "https://az.wikipedia.org/wiki/Veril%C9%99nl%C9%99r_elmi_(Data_Science)\n",
      "https://bn.wikipedia.org/wiki/%E0%A6%89%E0%A6%AA%E0%A6%BE%E0%A6%A4%E0%A7%8D%E0%A6%A4_%E0%A6%AC%E0%A6%BF%E0%A6%9C%E0%A7%8D%E0%A6%9E%E0%A6%BE%E0%A6%A8\n",
      "https://ca.wikipedia.org/wiki/Ci%C3%A8ncia_de_les_dades\n",
      "https://cs.wikipedia.org/wiki/Data_science\n",
      "https://de.wikipedia.org/wiki/Data_Science\n",
      "https://et.wikipedia.org/wiki/Andmeteadus\n",
      "https://el.wikipedia.org/wiki/%CE%95%CF%80%CE%B9%CF%83%CF%84%CE%AE%CE%BC%CE%B7_%CE%B4%CE%B5%CE%B4%CE%BF%CE%BC%CE%AD%CE%BD%CF%89%CE%BD\n",
      "https://es.wikipedia.org/wiki/Ciencia_de_datos\n",
      "https://eu.wikipedia.org/wiki/Datu_zientzia\n",
      "https://fa.wikipedia.org/wiki/%D8%B9%D9%84%D9%85_%D8%AF%D8%A7%D8%AF%D9%87%E2%80%8C%D9%87%D8%A7\n",
      "https://fr.wikipedia.org/wiki/Science_des_donn%C3%A9es\n",
      "https://ko.wikipedia.org/wiki/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4\n",
      "https://hy.wikipedia.org/wiki/%D5%8F%D5%BE%D5%B5%D5%A1%D5%AC%D5%B6%D5%A5%D6%80%D5%AB_%D5%A3%D5%AB%D5%BF%D5%B8%D6%82%D5%A9%D5%B5%D5%B8%D6%82%D5%B6\n",
      "https://hi.wikipedia.org/wiki/%E0%A4%86%E0%A4%81%E0%A4%95%E0%A4%A1%E0%A4%BC%E0%A4%BE_%E0%A4%B5%E0%A4%BF%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%A8\n",
      "https://id.wikipedia.org/wiki/Ilmu_data\n",
      "https://it.wikipedia.org/wiki/Scienza_dei_dati\n",
      "https://he.wikipedia.org/wiki/%D7%9E%D7%93%D7%A2_%D7%94%D7%A0%D7%AA%D7%95%D7%A0%D7%99%D7%9D\n",
      "https://kk.wikipedia.org/wiki/%D0%94%D0%B5%D1%80%D0%B5%D0%BA%D1%82%D0%B5%D1%80_%D1%82%D1%83%D1%80%D0%B0%D0%BB%D1%8B_%D2%93%D1%8B%D0%BB%D1%8B%D0%BC\n",
      "https://lv.wikipedia.org/wiki/Datu_m%C4%81c%C4%ABba\n",
      "https://mk.wikipedia.org/wiki/%D0%9D%D0%B0%D1%83%D0%BA%D0%B0_%D0%B7%D0%B0_%D0%BF%D0%BE%D0%B4%D0%B0%D1%82%D0%BE%D1%86%D0%B8\n",
      "https://ms.wikipedia.org/wiki/Sains_data\n",
      "https://my.wikipedia.org/wiki/%E1%80%A1%E1%80%81%E1%80%BB%E1%80%80%E1%80%BA%E1%80%A1%E1%80%9C%E1%80%80%E1%80%BA%E1%80%9E%E1%80%AD%E1%80%95%E1%80%B9%E1%80%95%E1%80%B6%E1%80%95%E1%80%8A%E1%80%AC\n",
      "https://nl.wikipedia.org/wiki/Datawetenschap\n",
      "https://ja.wikipedia.org/wiki/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9\n",
      "https://pl.wikipedia.org/wiki/Danologia\n",
      "https://pt.wikipedia.org/wiki/Ci%C3%AAncia_de_dados\n",
      "https://ru.wikipedia.org/wiki/%D0%9D%D0%B0%D1%83%D0%BA%D0%B0_%D0%BE_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85\n",
      "https://simple.wikipedia.org/wiki/Data_science\n",
      "https://fi.wikipedia.org/wiki/Datatiede\n",
      "https://ta.wikipedia.org/wiki/%E0%AE%A4%E0%AE%B0%E0%AE%B5%E0%AF%81_%E0%AE%85%E0%AE%B1%E0%AE%BF%E0%AE%B5%E0%AE%BF%E0%AE%AF%E0%AE%B2%E0%AF%8D\n",
      "https://th.wikipedia.org/wiki/%E0%B8%A7%E0%B8%B4%E0%B8%97%E0%B8%A2%E0%B8%B2%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%A1%E0%B8%B9%E0%B8%A5\n",
      "https://tr.wikipedia.org/wiki/Veri_bilimi\n",
      "https://uk.wikipedia.org/wiki/%D0%9D%D0%B0%D1%83%D0%BA%D0%B0_%D0%BF%D1%80%D0%BE_%D0%B4%D0%B0%D0%BD%D1%96\n",
      "https://ur.wikipedia.org/wiki/%DA%88%DB%8C%D9%B9%D8%A7_%D8%B3%D8%A7%D8%A6%D9%86%D8%B3\n",
      "https://vi.wikipedia.org/wiki/Khoa_h%E1%BB%8Dc_d%E1%BB%AF_li%E1%BB%87u\n",
      "https://zh-yue.wikipedia.org/wiki/%E6%95%B8%E6%93%9A%E7%A7%91%E5%AD%B8\n",
      "https://zh.wikipedia.org/wiki/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q2374463#sitelinks-wikipedia\n",
      "https://foundation.wikimedia.org/wiki/Privacy_policy\n",
      "https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute\n",
      "https://stats.wikimedia.org/#/en.wikipedia.org\n",
      "https://foundation.wikimedia.org/wiki/Cookie_statement\n",
      "https://wikimediafoundation.org/\n",
      "https://www.mediawiki.org/\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "\n",
    "r = urllib.request.urlopen('https://en.wikipedia.org/wiki/Data_science').read()\n",
    "soup = BeautifulSoup(r)\n",
    "\n",
    "links = soup.find_all('a')\n",
    "url_list = []\n",
    "num = 0\n",
    "\n",
    "for link in links:\n",
    "    linkUrl = str(link.get('href'))\n",
    "    if 'http' in linkUrl:\n",
    "        url_list.append(linkUrl)\n",
    "        num+=1\n",
    "        \n",
    "f = open('Q2.txt', 'w')\n",
    "\n",
    "f.write(str(num)+'\\n')\n",
    "print(num)\n",
    "for linkUrl in url_list:\n",
    "    f.write(linkUrl+'\\n')\n",
    "    print(linkUrl)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q3: Part 1 (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "1. Retrieve and parse multiple web pages. The text file \"urls.txt\" contains a list of webpages to be parsed. Each line in the text file corresponds to a url. Use BeautifulSoup to fetch each webpage and parse it as specified below. \n",
    "\n",
    "2. For each webpage document do the following:\n",
    "    1. Retrieve all text enclosed in paragraph tags. \n",
    "    2. Convert the text to lowercase. \n",
    "    3. Strip out punctuation. Note: if you use translate() with string.punctuation, then it may not strip out all characters. Use a regular expression involving \\W to strip out all non alpha-numeric characters.\n",
    "    4. Tokenize into words based on whitespace separation.\n",
    "\n",
    "3. Find the number of unique words in each webpage document. \n",
    "\n",
    "4. Find the Length of each webpage document. The length of a document is defined as the total number of words in the document (not just unique words).\n",
    "\n",
    "5. For each of the following words: “statistics”, “analytics”, “data”, and “science”, \n",
    "    a. Find Term Frequency (tf). \n",
    "    The term frequency (tf) of a term (word) is defined as the number of times that term t occurs in document d, \n",
    "    divided by the total number of words in the document. \n",
    "    The tf of a word depends on the document under consideration. \n",
    "    \n",
    "    b. Find Inverse Document Frequency (idf).\n",
    "    The inverse document frequency of a word is the logarithmically scaled inverse fraction of the documents that \n",
    "    contain the word, obtained by dividing the total number of documents by the number of documents containing the \n",
    "    term, and then taking the logarithm of that ratio.\n",
    "    The idf of a word doesn't depend on any documnet in which the word is present. \n",
    "    To calculate the idf, you will have to use the log function. The base for the log function must be e.\n",
    "    \n",
    "    c. Find tf-idf. \n",
    "    The tf-idf of a word is the product of the term frequency of the word in document d, and its inverse document \n",
    "    frequency. \n",
    "    The tf-idf of a word depends on the document under consideration. \n",
    "    \n",
    "    Reference: https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "    \n",
    "The output should be written to (1) this Notebook and (2) an output file named \"Q3_Part1.txt\".\n",
    "\n",
    "The format of the output file is as shown below:\n",
    "\n",
    "1. Number of unique words in documents: [702, 723, 280]\n",
    "\n",
    "2. Length of documents: [1711, 1928, 563]\n",
    "\n",
    "3. tf\n",
    "    statistics: [0.0070134424313267095, 0.0025933609958506223, 0.0]\n",
    "    analytics: [0.0029222676797194622, 0.0031120331950207467, 0.0]\n",
    "    data: [0.056107539450613676, 0.05446058091286307, 0.0]\n",
    "    science: [0.03798947983635301, 0.011410788381742738, 0.028419182948490232]\n",
    "    \n",
    "4. idf \n",
    "    statistics: 0.510825623766\n",
    "    analytics: 0.510825623766\n",
    "    data: 0.223143551314\n",
    "    science: 0.0\n",
    "    \n",
    "5. tf-idf\n",
    "    statistics: [0.0028437061936282715, 0.0010515173965460695, 0.0]\n",
    "    analytics: [0.0011848775806784465, 0.0012618208758552834, 0.0]\n",
    "    data: [0.022749649549026172, 0.022081865327467459, 0.0]\n",
    "    science: [0.0, 0.0, 0.0]\n",
    "   \n",
    "The above values are for the first three webpage urls given in the file \"urls.txt\". The number of unique words in documents, average length of documents, tf and tf-idf values for the four words, must be in the order of the urls given in \"urls.txt\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Number of unique words in documents: [473, 1975, 727, 930, 1170]\n",
      "2. Length of documents: [1049, 6306, 1950, 2380, 3681]\n",
      "3. tf statistics: [0.017159199237368923, 0.003171582619727244, 0.01282051282051282, 0.01050420168067227, 0.010051616408584624] analytics: [0.0009532888465204957, 0.0030130034887408817, 0.01282051282051282, 0.010924369747899159, 0.008149959250203748] data: [0.06482364156339371, 0.05629559150015858, 0.23743589743589744, 0.19453781512605042, 0.17440912795436023] science: [0.03813155386081983, 0.008087535680304472, 0.03743589743589744, 0.03235294117647059, 0.037489812550937245] \n",
      "4. idf statistics: 0.22314355131420976 analytics: 0.0 data: 0.22314355131420976 science: 0.0 \n",
      "5. tf-idf statistics:  [0.0038289646555345813, 0.0007077182090523621, 0.002860814760438587, 0.002343944866745901, 0.002242953381859756] analytics:  [0.0, 0.0, 0.0, 0.0, 0.0] data:  [0.014464977587575084, 0.012561998210679427, 0.052982289363322624, 0.04340985893213409, 0.03891827219335036] science:  [0.0, 0.0, 0.0, 0.0, 0.0] "
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import math\n",
    "\n",
    "f = open('urls.txt', 'r')\n",
    "url = f.readline()\n",
    "word_list = []\n",
    "unique_words = 0\n",
    "tf_words=[\"statistics\", \"analytics\", \"data\", \"science\"]\n",
    "tf_freq=[0, 0, 0, 0]\n",
    "idf_docfreq=[0, 0, 0, 0]\n",
    "idf_flag=[0, 0, 0, 0]\n",
    "\n",
    "unique_words_res = []\n",
    "length_res = []\n",
    "tf_res = [[0 for col in range(5)] for row in range(4)]\n",
    "idf_res = []\n",
    "cnt = 0\n",
    "\n",
    "while url:\n",
    "    r = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(r)\n",
    "    paragraphs = soup.find_all('p')\n",
    "    unique_words = 0\n",
    "    words = 0\n",
    "    word_list = []\n",
    "    for i in range(0, 4, 1):\n",
    "        idf_flag[i] = 0\n",
    "    for p in paragraphs:\n",
    "        pText = p.get_text()\n",
    "        pText = pText.lower()\n",
    "        pattern = re.compile(r\"\\W\")\n",
    "        for match in re.finditer(pattern, pText):\n",
    "            if match.group() != ' ':\n",
    "                pText = pText.replace(match.group(), ' ')\n",
    "        pText = pText.split(' ')\n",
    "        for w in pText:\n",
    "            if w != '':\n",
    "                words+=1\n",
    "                if w not in word_list:\n",
    "                    word_list.append(w)\n",
    "                    unique_words+=1\n",
    "                if w in tf_words:\n",
    "                    for i in range(0, 4, 1):\n",
    "                        if tf_words[i] == w:\n",
    "                            tf_freq[i] += 1\n",
    "                            if idf_flag[i] == 0:\n",
    "                                idf_flag[i] = 1\n",
    "                                idf_docfreq[i]+=1\n",
    "\n",
    "    unique_words_res.append(unique_words)\n",
    "    length_res.append(words)\n",
    "    for i in range(0, 4, 1):\n",
    "        tf_res[i][cnt] = tf_freq[i]/words\n",
    "    cnt+=1\n",
    "    \n",
    "    url = f.readline()\n",
    "\n",
    "for i in range(0, 4, 1):\n",
    "    idf_res.append(math.log(5/idf_docfreq[i]))\n",
    "    \n",
    "f.close()\n",
    "f = open('Q3_Part1.txt', 'w')\n",
    "f.write(\"1. Number of unique words in documents: \")\n",
    "f.write(str(unique_words_res)+'\\n')\n",
    "print(\"1. Number of unique words in documents:\", unique_words_res)\n",
    "f.write(\"2. Length of documents: \")\n",
    "f.write(str(length_res)+'\\n')\n",
    "print(\"2. Length of documents:\", length_res)\n",
    "f.write(\"3. tf \")\n",
    "print(\"3. tf \", end=\"\")\n",
    "for i in range(0, 4, 1):\n",
    "    f.write(tf_words[i]+\": \")\n",
    "    f.write(str(tf_res[i])+' ')\n",
    "    print(tf_words[i]+':', str(tf_res[i]), end=\" \")\n",
    "        \n",
    "f.write(\"\\n4. idf \")\n",
    "print(\"\\n4. idf \", end=\"\")\n",
    "for i in range(0, 4, 1):\n",
    "    f.write(tf_words[i]+\": \")\n",
    "    f.write(str(idf_res[i])+' ')\n",
    "    print(tf_words[i]+':', str(idf_res[i]), end=\" \")\n",
    "    \n",
    "tf_idf_res = [[0 for col in range(5)] for row in range(4)]\n",
    "for i in range(0, 4, 1):\n",
    "    for j in range(0, 5, 1):\n",
    "        tf_idf_res[i][j] = tf_res[i][j]*idf_res[i]\n",
    "f.write(\"\\n5. tf-idf \")\n",
    "print(\"\\n5. tf-idf\", end=\" \")\n",
    "for i in range(0, 4, 1):\n",
    "    f.write(tf_words[i]+\": \")\n",
    "    f.write(str(tf_idf_res[i])+' ')\n",
    "    print(tf_words[i]+\": \", str(tf_idf_res[i]), end=\" \")\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q3: Part 2 (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Repeat Part 1, but first remove the stop words given in the file \"stop_words.txt\".  \n",
    "The ouput for Part 2 should have the same format as Part 1, and should be written to  (1) this Notebook and (2) an output file named \"Q3_Part2.txt\".\n",
    "Note: The length of document, in this case, will not include stop words. Similarly, the number of unique words in documents, and the calculation of tf, idf, tf-idf should be done after removing the stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Example output for the file \"urls.txt\" (Note that this is just an example (NOT correct answer).)\n",
    "\n",
    "1. Number of unique words in documents: [596, 600, 231]\n",
    "\n",
    "2. Length of documents: [1020, 1079, 357]  \n",
    "\n",
    "3. tf\n",
    "    statistics: [0.011764705882352941, 0.004633920296570899, 0.0]\n",
    "    analytics: [0.004901960784313725, 0.005560704355885079, 0.0]\n",
    "    data: [0.09411764705882353, 0.09731232622798888, 0.0]\n",
    "    science: [0.06372549019607843, 0.020389249304911955, 0.04481792717086835]\n",
    "    \n",
    "4. idf \n",
    "    statistics: 0.405465108108\n",
    "    analytics: 0.405465108108\n",
    "    data: 0.405465108108\n",
    "    science: 0.0\n",
    "    \n",
    "5. tf-idf\n",
    "    statistics: [0.0047701777424489925, 0.0018788929940137366, 0.0]\n",
    "    analytics: [0.0019875740593537469, 0.002254671592816484, 0.0]\n",
    "    data: [0.03816142193959194, 0.039456752874288473, 0.0]\n",
    "    science: [0.0, 0.0, 0.0] \n",
    "      \n",
    "The above values are for the first three webpage urls given in the file \"urls.txt\". The number of unique words in documents, average length of documents, tf and tf-idf values for the four words, must be in the order of the urls given in \"urls.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Number of unique words in documents: [389, 1794, 606, 825, 1017]\n",
      "2. Length of documents: [644, 3837, 1102, 1521, 2051]\n",
      "3. tf statistics: [0.027950310559006212, 0.005212405525149857, 0.022686025408348458, 0.01643655489809336, 0.01803998049731838] analytics: [0.0015527950310559005, 0.0049517852488923635, 0.022686025408348458, 0.017094017094017096, 0.01462701121404193] data: [0.10559006211180125, 0.09252019807140996, 0.42014519056261346, 0.304404996712689, 0.3130180399804973] science: [0.062111801242236024, 0.013291634089132134, 0.0662431941923775, 0.05062458908612755, 0.06728425158459288] \n",
      "4. idf statistics: 0.22314355131420976 analytics: 0.0 data: 0.22314355131420976 science: 0.0 \n",
      "5. tf-idf statistics:  [0.006236931558471701, 0.0011631146797717476, 0.005062240274823271, 0.0036677112313315214, 0.004025505313810708] analytics:  [0.0, 0.0, 0.0, 0.0, 0.0] data:  [0.02356174144311532, 0.02064528556594852, 0.09375268988972697, 0.06792601200425977, 0.06984795706666147] science:  [0.0, 0.0, 0.0, 0.0, 0.0] "
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import re\n",
    "import math\n",
    "\n",
    "f = open('urls.txt', 'r')\n",
    "url = f.readline()\n",
    "word_list = []\n",
    "unique_words = 0\n",
    "tf_words=[\"statistics\", \"analytics\", \"data\", \"science\"]\n",
    "tf_freq=[0, 0, 0, 0]\n",
    "idf_docfreq=[0, 0, 0, 0]\n",
    "idf_flag=[0, 0, 0, 0]\n",
    "\n",
    "unique_words_res = []\n",
    "length_res = []\n",
    "tf_res = [[0 for col in range(5)] for row in range(4)]\n",
    "idf_res = []\n",
    "cnt = 0\n",
    "\n",
    "f2 = open('stop_words.txt', 'r')\n",
    "stop_list = f2.readlines()\n",
    "for i in range(0, len(stop_list), 1):\n",
    "    stop_list[i] = stop_list[i][:len(stop_list[i])-1]\n",
    "f2.close()\n",
    "\n",
    "while url:\n",
    "    r = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(r)\n",
    "    paragraphs = soup.find_all('p')\n",
    "    unique_words = 0\n",
    "    words = 0\n",
    "    word_list = []\n",
    "    for i in range(0, 4, 1):\n",
    "        idf_flag[i] = 0\n",
    "    for p in paragraphs:\n",
    "        pText = p.get_text()\n",
    "        pText = pText.lower()\n",
    "        pattern = re.compile(r\"\\W\")\n",
    "        for match in re.finditer(pattern, pText):\n",
    "            if match.group() != ' ':\n",
    "                pText = pText.replace(match.group(), ' ')\n",
    "        pText = pText.split(' ')\n",
    "        for w in pText:\n",
    "            if w not in stop_list:\n",
    "                if w != '':\n",
    "                    words+=1\n",
    "                    if w not in word_list:\n",
    "                        word_list.append(w)\n",
    "                        unique_words+=1\n",
    "                    if w in tf_words:\n",
    "                        for i in range(0, 4, 1):\n",
    "                            if tf_words[i] == w:\n",
    "                                tf_freq[i] += 1\n",
    "                                if idf_flag[i] == 0:\n",
    "                                    idf_flag[i] = 1\n",
    "                                    idf_docfreq[i]+=1\n",
    "\n",
    "    unique_words_res.append(unique_words)\n",
    "    length_res.append(words)\n",
    "    for i in range(0, 4, 1):\n",
    "        tf_res[i][cnt] = tf_freq[i]/words\n",
    "    cnt+=1\n",
    "    \n",
    "    url = f.readline()\n",
    "\n",
    "for i in range(0, 4, 1):\n",
    "    idf_res.append(math.log(5/idf_docfreq[i]))\n",
    "    \n",
    "f.close()\n",
    "f = open('Q3_Part2.txt', 'w')\n",
    "f.write(\"1. Number of unique words in documents: \")\n",
    "f.write(str(unique_words_res)+'\\n')\n",
    "print(\"1. Number of unique words in documents:\", unique_words_res)\n",
    "f.write(\"2. Length of documents: \")\n",
    "f.write(str(length_res)+'\\n')\n",
    "print(\"2. Length of documents:\", length_res)\n",
    "f.write(\"3. tf \")\n",
    "print(\"3. tf \", end=\"\")\n",
    "for i in range(0, 4, 1):\n",
    "    f.write(tf_words[i]+\": \")\n",
    "    f.write(str(tf_res[i])+' ')\n",
    "    print(tf_words[i]+':', str(tf_res[i]), end=\" \")\n",
    "        \n",
    "f.write(\"\\n4. idf \")\n",
    "print(\"\\n4. idf \", end=\"\")\n",
    "for i in range(0, 4, 1):\n",
    "    f.write(tf_words[i]+\": \")\n",
    "    f.write(str(idf_res[i])+' ')\n",
    "    print(tf_words[i]+':', str(idf_res[i]), end=\" \")\n",
    "    \n",
    "tf_idf_res = [[0 for col in range(5)] for row in range(4)]\n",
    "for i in range(0, 4, 1):\n",
    "    for j in range(0, 5, 1):\n",
    "        tf_idf_res[i][j] = tf_res[i][j]*idf_res[i]\n",
    "f.write(\"\\n5. tf-idf \")\n",
    "print(\"\\n5. tf-idf\", end=\" \")\n",
    "for i in range(0, 4, 1):\n",
    "    f.write(tf_words[i]+\": \")\n",
    "    f.write(str(tf_idf_res[i])+' ')\n",
    "    print(tf_words[i]+\": \", str(tf_idf_res[i]), end=\" \")\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
